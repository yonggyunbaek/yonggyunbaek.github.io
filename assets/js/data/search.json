[ { "title": "csv파일 spark이용해서 kudu저장", "url": "/posts/csv_spark_kudu/", "categories": "cloudera", "tags": "spark, kudu", "date": "2023-04-18 00:00:00 +0900", "snippet": "csv –&gt; spark –&gt; kudu1. Load and prepare the CSV datakudutest.csvA,B,NUM,DTa,b,1,2023-02-27 14:52:06c,d,2,2023-02-27 15:00:00e,f,3,2023-02-27 16:00:00HADOOP_USER_NAME=hdfs spark3-shell --jars /opt/cloudera/parcels/CDH/lib/kudu/kudu-spark3_2.12.jarimport org.apache.spark.sql.types._val kdtest = spark.sqlContext.read.format(\"csv\").option(\"header\",\"true\").option(\"inferSchema\",\"true\").load(\"/tmp/kudutest.csv\")//val kdtest = spark.sqlContext.read.format(\"csv\").option(\"header\",\"true\").schema(customschema).load(\"/tmp/kudutest.csv\")kdtest.printSchemakdtest.createOrReplaceTempView(\"kdtest\")/*scala&gt; spark.sql(\"SELECT count(*) FROM kdtest\").show()+--------+|count(1)|+--------+| 3|+--------+scala&gt; spark.sql(\"SELECT * FROM kdtest\").show()+---+---+---+-------------------+| A| B|NUM| DT|+---+---+---+-------------------+| a| b| 1|2023-02-27 14:52:06|| c| d| 2|2023-02-27 15:00:00|| e| f| 3|2023-02-27 16:00:00|+---+---+---+-------------------+*/2. Load and prepare the Kudu table1번 쉘 세션 유지한 상태에서 추가 수행import collection.JavaConverters._import org.apache.kudu.client._import org.apache.kudu.spark.kudu._val kuduContext = new KuduContext(\"&lt;hostname&gt;:7051\", spark.sparkContext)kuduContext.tableExists(\"csvsparkkudu\")if(kuduContext.tableExists(\"csvsparkkudu\")) {\tkuduContext.deleteTable(\"csvsparkkudu\")}//kudu key column --&gt; nullable = falseval customschema = StructType(Array(StructField(\"A\",StringType, nullable = true),StructField(\"B\",StringType, nullable = false),StructField(\"NUM\", IntegerType, nullable = false),StructField(\"DT\", StringType, nullable = true)))kuduContext.createTable(\"default.csvsparkkudu\", customschema, /* primary key */ Seq(\"B\", \"NUM\"), new CreateTableOptions() .setNumReplicas(3) .addHashPartitions(List(\"B\").asJava, 4))kuduContext.insertRows(kdtest, \"default.csvsparkkudu\")// Create a DataFrame that points to the Kudu table we want to query.val csvsparkkudu = spark.read\t.option(\"kudu.master\", \"&lt;hostname&gt;:7051\")\t.option(\"kudu.table\", \"default.csvsparkkudu\")\t// We need to use leader_only because Kudu on Docker currently doesn't\t// support Snapshot scans due to `--use_hybrid_clock=false`.\t.option(\"kudu.scanLocality\", \"leader_only\")\t.format(\"kudu\").loadcsvsparkkudu.createOrReplaceTempView(\"default.csvsparkkudu\")spark.sql(\"SELECT count(*) FROM default.csvsparkkudu\").show()spark.sql(\"SELECT * FROM default.csvsparkkudu LIMIT 5\").show()3. Impala로 확인INVALIDATE METADATA 수행하면 external table 생성되는 것으로 보임 (추가 확인 필요)" }, { "title": "hive create table error(field.delim)", "url": "/posts/hive_field_delim_error/", "categories": "cloudera", "tags": "hive", "date": "2023-04-05 00:00:00 +0900", "snippet": "작업배경 테이블 migration 진행 기존 클러스터에서 show create 이용해서 ddl 추출 후 신규 클러스터에서 실행 Error: Error while compiling statement: FAILED: ParseException line 8:1 cannot recognize input near ‘’,\\n’’ ‘serialization’ ‘.’ in table properties list (state=42000,code=40000)1. 추출한 ddl 확인CREATE EXTERNAL TABLE `default`.`customers_4`( `name` string, `purchanse_count` int)ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'WITH SERDEPROPERTIES ( 'field.delim'='^G', 'serialization.format'='\\n')STORED AS INPUTFORMAT 'org.apache.hadoop.mapred.TextInputFormat'OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'LOCATION '/warehouse/tablespace/external/hive/customers_4'TBLPROPERTIES ( 'bucketing_version'='2', 'external.table.purge'='true', 'transient_lastDdlTime'='1680698916');2. 실제 데이터 구분자 확인a&amp;#7;1b&amp;#7;2c&amp;#7;33. 구분자에 대한 정보 확인참고: https://www.fileformat.info/info/unicode/char/0007/index.htm C/C++/Java source code\t“\\u0007”4. DDL 수정CREATE EXTERNAL TABLE `default`.`customers_4`( `name` string, `purchanse_count` int)ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'WITH SERDEPROPERTIES (------------------------------ 'field.delim'='\\u0007',------------------------------ 'serialization.format'='\\n')STORED AS INPUTFORMAT 'org.apache.hadoop.mapred.TextInputFormat'OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'LOCATION '/warehouse/tablespace/external/hive/customers_4'TBLPROPERTIES ( 'bucketing_version'='2', 'external.table.purge'='true', 'transient_lastDdlTime'='1680698916');5. DDL 실행 후 show create로 확인1) beeline cli 출력+----------------------------------------------------+| createtab_stmt |+----------------------------------------------------+| CREATE EXTERNAL TABLE `default`.`customers_4`( || `name` string, || `purchanse_count` int) || ROW FORMAT SERDE || 'org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe' || WITH SERDEPROPERTIES ( || 'field.delim'='', || 'serialization.format'='\\n') || STORED AS INPUTFORMAT || 'org.apache.hadoop.mapred.TextInputFormat' || OUTPUTFORMAT || 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat' || LOCATION || 'hdfs://&lt;host&gt;:8020/warehouse/tablespace/external/hive/customers_4' || TBLPROPERTIES ( || 'bucketing_version'='2', || 'external.table.purge'='true', || 'transient_lastDdlTime'='1680703862') |+----------------------------------------------------+2) beeline 파일 output+----------------------------------------------------+| createtab_stmt |+----------------------------------------------------+| CREATE EXTERNAL TABLE `default`.`customers_4`( || `name` string, || `purchanse_count` int) || ROW FORMAT SERDE || 'org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe' || WITH SERDEPROPERTIES ( || 'field.delim'='^G', || 'serialization.format'='\\n') || STORED AS INPUTFORMAT || 'org.apache.hadoop.mapred.TextInputFormat' || OUTPUTFORMAT || 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat' || LOCATION || 'hdfs://&lt;host&gt;:8020/warehouse/tablespace/external/hive/customers_4' || TBLPROPERTIES ( || 'bucketing_version'='2', || 'external.table.purge'='true', || 'transient_lastDdlTime'='1680703862') |+----------------------------------------------------+3) hue에서 실행결과CREATE EXTERNAL TABLE `default`.`customers_4`(`name` string,`purchanse_count` int)ROW FORMAT SERDE'org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'WITH SERDEPROPERTIES ('field.delim'='&amp;#7;','serialization.format'='\\n')STORED AS INPUTFORMAT'org.apache.hadoop.mapred.TextInputFormat'OUTPUTFORMAT'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'LOCATION'hdfs://&lt;host&gt;:8020/warehouse/tablespace/external/hive/customers_4'TBLPROPERTIES ('bucketing_version'='2','external.table.purge'='true','transient_lastDdlTime'='1680703862')" }, { "title": "hive metastore에서 테이블 리스트 추출", "url": "/posts/hive_metastore_db_tables/", "categories": "cloudera", "tags": "hive, mysql", "date": "2023-03-19 00:00:00 +0900", "snippet": "metastore 가 mariadb로 설치된 상태1. db_name, tbl_name 형태로 추출하기select name AS db_name,tbl_name FROM TBLS INNER JOIN DBS ON TBLS.DB_ID = DBS.DB_ID WHERE DBS.name='default'1) 응용(테이블 리스트 추출해서 msck repair 하는 쿼리로 만들기)mysql -u${user} -p${password} metastore -N -e \"select concat('msck repair table ', DBS.name,'.',TBLS.tbl_name,' drop partitions;') FROM TBLS INNER JOIN DBS ON TBLS.DB_ID = DBS.DB_ID WHERE DBS.name='&lt;db_name&gt;';\" &gt; hive_tables.sql# -N: header 생략출력 내용 예시vi hive_tables.sqlmsck repair table default.customers drop partitions;msck repair table default.customers_2 drop partitions;" }, { "title": "hive,impala - jdbc,odbc 연결", "url": "/posts/hive,impala_jdbc,odbc/", "categories": "cloudera", "tags": "impala, hive", "date": "2023-02-07 00:00:00 +0900", "snippet": "1. cloudera hive,impala JDBC,ODBC 다운로드https://www.cloudera.com/downloads/connectors/impala/jdbc/2-6-29.html.html https://www.cloudera.com/downloads/connectors/impala/odbc/2-6-17.html https://www.cloudera.com/downloads/connectors/hive/jdbc/2-6-21.html https://www.cloudera.com/downloads/connectors/hive/odbc/2-6-13.html hive, impala 서비스 ldap 설정 상태1) Impala JDBCpip3 install JayDebeAPIimport jaydebeapiconn = jaydebeapi.connect(\"com.cloudera.impala.jdbc41.DataSource\", \"jdbc:impala://&lt;hostname&gt;:21050/;AuthMech=3;\", {'UID': \"&lt;user&gt;\", 'PWD': \"&lt;passwd&gt;\"}, '/home/cloudera/ImpalaJDBC41.jar')curs = conn.cursor()curs.execute(\"show databases\")curs.fetchall()curs.close()conn.close()​​import jaydebeapiconn = jaydebeapi.connect(\"com.cloudera.hive.jdbc.HS2DataSource\",\"jdbc:hive2://&lt;host&gt;:10000/;AuthMech=3\",{'UID':\"&lt;user&gt;\", 'PWD':\"&lt;passwd&gt;\"},'/root/HiveJDBC-2.6.21.1025/HiveJDBC41.jar')# /root/HiveJDBC-2.6.21.1025crsr=conn.cursor()#crsr.execute('select * from test01.hive_exteral_test_01;')crsr.execute('show databases;')print(crsr.fetchall())2) ODBC ODBC 설치 yum --nogpgcheck localinstall ClouderaHiveODBC-2.6.13.1013-1.x86_64.rpmyum --nogpgcheck localinstall ClouderaImpalaODBC-2.6.11.1011-1.x86_64.rpm pyodbc 설치 pip3 install pyodbc test.py 커넥션 테스트# hive ODBCimport pyodbcconnString = pyodbc.connect('Driver=/opt/cloudera/hiveodbc/lib/64/libclouderahiveodbc64.so;Host=&lt;hostname&gt;;Port=10000;AuthMech=3;UID=&lt;user&gt;;PWD=&lt;passwd&gt;;',autocommit=True)crsr=connString.cursor()crsr.execute('show tables;')print(crsr.fetchall())------------------------------$ python3 test.py[('ranger_atlas_test', ), ('ranger_atlas_test_2', )]# impala ODBCimport pyodbcconnString = pyodbc.connect('Driver=/opt/cloudera/impalaodbc/lib/64/libclouderaimpalaodbc64.so;Host=&lt;hostname&gt;;Port=10000;AuthMech=3;UID=&lt;user&gt;;PWD=&lt;passwd&gt;;',autocommit=True)crsr=connString.cursor()crsr.execute('select * from testtable;')print(crsr.fetchall())------------------------------$ python3 test.py[(1, 1)]" }, { "title": "YARN scheduler (추가 예정)", "url": "/posts/yarn_scheduler/", "categories": "cloudera", "tags": "yarn, yarn-queue-manager", "date": "2022-12-21 00:00:00 +0900", "snippet": "1. Fair scheduler자원 경합시 설정한 비율만큼 할당 받음2. capacity scheduler미리 예약해둔 queue만큼 자원 사용" }, { "title": "kudu migration 방법 (추가 예정)", "url": "/posts/kudu_mig/", "categories": "cloudera", "tags": "kudu, impala, spark", "date": "2022-12-21 00:00:00 +0900", "snippet": "1. kudu table copycli kudu command2. impala - insert into selectkudu external 테이블 생성(소스 클러스터 kudu master 지정)하여 insert into select2. spark backup toolbackup tool 이용한 HDFS backup &amp; restore - 변경분 백업 가능" }, { "title": "impala 통계 정보에 대하여", "url": "/posts/impalastats/", "categories": "cloudera", "tags": "impala", "date": "2022-12-13 00:00:00 +0900", "snippet": "impala는 테이블 사이즈가 큰지 작은지, distinct 값들이 많은지 적은지 등에 대한 정보가 있다면 join 쿼리나 insert 작업을 적절하게 구조화하고 병렬화할 수 있다.1. COMPUTE STATSCOMPUTE STATS [db_name.]table_name [ ( column_list ) ] -- column_list 여러개면 ,로 구분 컬럼 리스트 미지정시 모든 컬럼에 대한 통계를 계산한다. 지정한 컬럼이 통계 계산을 지원하지 않는 type이거나 partitioning 컬럼일 경우 에러가 발생한다.1) compute incremental statsCOMPUTE INCREMENTAL STATS [db_name.]table_name [PARTITION (partition_spec)] incremental 만 partition지정 가능하다 compute stats와 compute incremental stats를 한 테이블에 같이 사용하지 않는다. 둘 중 하나로 전환할 경우 drop stats 실행 후 전환한다. 2) sampling (문서만 확인 - test 필요) 설정 enable 필요함 config 변경 –enable_stats_extrapolation 또는 특정 테이블만 설정ALTER TABLE mytable test_table SET TBLPROPERTIES(\"impala.enable.stats.extrapolation\"=\"true\")COMPUTE STATS [db_name.]table_name [ ( column_list ) ] [TABLESAMPLE SYSTEM(percentage) [REPEATABLE(seed)]]-- 10%는 TABLESAMPLE SYSTEM(10) -- seed 임의의 양의 정수 - 쿼리가 다시 실행될 때 매번 동일한 데이터 파일 집합을 선택하도록 하는 옵션2. ALTER1) row count updatealter table &lt;TABLE_NAME&gt; set tblproperties('numRows'=&lt;rowcount&gt;, 'STATS_GENERATED_VIA_STATS_TASK'='true');2) column stats updatealter table &lt;TABLE_NAME&gt; set set column stats &lt;columnname&gt; ('numDVs'='&lt;DV&gt;', 'numNulls'='&lt;numN&gt;', 'maxsize'='&lt;Max_size&gt;, 'avgsize'='&lt;Avg_size&gt;'); column type에 따라서 maxsize, avgsize 는 update 할 수 없다(고정값) 3. 통계정보 확인show table stats [db_name.]table_nameshow column stats [db_name.]table_name-- '-1'이 있으면 통계정보 없는 상태또는 쿼리 실행전 explain 했을때 통계정보가 없다는 warning이 발생하지 않으면 됨" }, { "title": "블로그 포스팅 방법", "url": "/posts/first/", "categories": "Blog", "tags": "Blog, jekyll, Github", "date": "2022-07-03 00:00:00 +0900", "snippet": "1. _posts 위치에 .md 파일 작성머릿말 --- 로 표시---title: \"블로그 포스팅 방법\"excerpt: \"_posts 위치에 .md 파일 작성 후 업로드\"categories: [Blog]tag: [Blog, jekyll, Github]toc: truetoc_sticky: truedate: 2022-07-03last_modifed_at: 2022-07-03---본문내용 작성마크다운 문법 미리보기 로컬에서 확인2. 작성한 .md 파일 git push" } ]
